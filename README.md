# Transformer from Scratch

A from-scratch implementation of the Transformer model architecture introduced in the paper ["Attention Is All You Need"](https://arxiv.org/abs/1706.03762) by Vaswani et al. (2017).

## Features

- Pure Python/PyTorch implementation
- Complete transformer architecture including:
  - Multi-head attention
  - Positional encoding
  - Encoder-decoder structure
  - Layer normalization
- Training and inference scripts
- Language Translator for English to Italy 


